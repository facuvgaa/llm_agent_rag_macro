import uuid
from langgraph.graph import START, MessagesState, StateGraph
from langgraph.checkpoint.memory import MemorySaver
from langchain_ollama.llms import OllamaLLM
from langchain_core.messages import HumanMessage, AIMessage
from models.qdrant_schemas import qdrant_docs, qdrant_conversations
from models.redis_cache import redis_cache
import os


OLLAMA_URL = "http://llm_service:11434"

# Modelo LLM
llm = OllamaLLM(model="llama3.2:3b", base_url=OLLAMA_URL)

# Grafo de estados
workflow = StateGraph(state_schema=MessagesState)

def call_model(state: MessagesState):
    # Extraer el contenido del √∫ltimo mensaje del usuario
    last_message = state["messages"][-1]
    # Los mensajes son objetos Message de LangChain, acceder al atributo content
    prompt = last_message.content if hasattr(last_message, 'content') else str(last_message)
    
    # Invocar el LLM con el prompt
    response = llm.invoke(prompt)
    
    # Retornar en formato de mensaje usando AIMessage de LangChain
    return {"messages": [AIMessage(content=response)]}

workflow.add_edge(START, "model")
workflow.add_node("model", call_model)

# Persistencia de memoria
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)

# Funci√≥n principal
def generate_answer(question: str, thread_id: str = None) -> str:
    if thread_id is None:
        thread_id = str(uuid.uuid4())

    # Obtener historial de conversaci√≥n previo
    conversation_history = redis_cache.get_conversation_cache(thread_id)
    context_historico = ""
    
    if conversation_history and len(conversation_history) > 0:
        # Construir contexto hist√≥rico de las √∫ltimas 3 interacciones
        ultimas_interacciones = conversation_history[-3:]
        context_parts = []
        for msg in ultimas_interacciones:
            if msg.get("question"):
                context_parts.append(f"Pregunta anterior: {msg['question']}")
            if msg.get("answer"):
                context_parts.append(f"Respuesta anterior: {msg['answer']}")
        
        if context_parts:
            context_historico = "\n".join(context_parts)
            print(f"üìú Contexto hist√≥rico encontrado: {len(conversation_history)} mensajes previos")

    cached_answer = redis_cache.get_cached_answer(question)

    if cached_answer:
        print(f"respuesta obtenida de cache: {cached_answer}")
        redis_cache.save_to_conversation(thread_id, question, cached_answer)
        return cached_answer
    
    # Detectar saludos simples y responder directamente sin usar LLM
    question_lower = question.lower().strip()
    saludos_simples = ["hola", "hola, como estas", "como estas", "buenos d√≠as", "buenas tardes", "buenas noches", "hi", "hello"]
    
    # Respuestas directas para preguntas comunes sin contexto
    if question_lower in saludos_simples:
        answer = "Hola, ¬øen qu√© puedo ayudarte?"
        redis_cache.cache_answer(question, answer)
        redis_cache.save_to_conversation(thread_id, question, answer)
        return answer
    
    # Si pregunta si puede buscar en internet o cosas fuera del contexto del banco
    preguntas_fuera_contexto = ["podes buscar en internet", "puedes buscar en internet", "buscar en internet", "busca en google"]
    if any(pregunta in question_lower for pregunta in preguntas_fuera_contexto):
        answer = "No, no puedo buscar en internet. Solo puedo responder con la informaci√≥n disponible en los documentos del banco."
        redis_cache.cache_answer(question, answer)
        redis_cache.save_to_conversation(thread_id, question, answer)
        return answer
    
    # Si pregunta si es un agente del banco
    if "agente" in question_lower and ("banco" in question_lower or "macro" in question_lower):
        answer = "S√≠, soy un asistente virtual del Banco Macro. ¬øEn qu√© puedo ayudarte?"
        redis_cache.cache_answer(question, answer)
        redis_cache.save_to_conversation(thread_id, question, answer)
        return answer
    
    # Detectar preguntas espec√≠ficas sobre token (trabo, bloqueado, problema, etc.)
    question_lower = question.lower().strip()
    es_pregunta_token = any(palabra in question_lower for palabra in ["token", "trabo", "traba", "bloqueado", "no funciona", "no responde", "problema con", "arreglar", "generar", "nuevo token", "crear token", "activar token"])
    
    # Detectar si menciona cambiar dispositivo/celular - esto es importante para token
    menciona_cambio_dispositivo = any(palabra in question_lower for palabra in ["cambie", "cambio", "cambiar", "nuevo celular", "nuevo dispositivo", "otro celular", "otro dispositivo"])
    
    # Detectar si pregunta espec√≠ficamente sobre ir al cajero autom√°tico
    pregunta_sobre_cajero = any(palabra in question_lower for palabra in ["cajero", "quiero ir", "ir al cajero", "cajero automatico", "cajero autom√°tico", "banelco"])
    
    # Detectar preguntas vagas que necesitan contexto adicional
    preguntas_vagas = ["me podes guiar", "me guias", "como hago", "podes ayudarme", "ayudame", "guia", "instrucciones"]
    es_pregunta_vaga = any(pregunta in question_lower for pregunta in preguntas_vagas)
    
    # Si pregunta espec√≠ficamente sobre ir al cajero autom√°tico, buscar informaci√≥n del cajero
    if pregunta_sobre_cajero:
        # Buscar espec√≠ficamente sobre el proceso en el cajero autom√°tico
        # Incluir "Dirigite" que es parte del texto clave del documento
        search_query = "Dirigite cajero automatico banelco tarjeta debito claves generacion token"
        print(f"üîç Pregunta sobre cajero autom√°tico detectada, buscando: {search_query}")
    # Si pregunta espec√≠fica sobre token o cambio de dispositivo relacionado con token, mejorar la b√∫squeda
    elif es_pregunta_token or (menciona_cambio_dispositivo and "token" in question_lower):
        # Si la pregunta contiene "token", buscar directamente con t√©rminos clave de token
        # Usar t√©rminos espec√≠ficos de activaci√≥n y generaci√≥n para mejorar la relevancia
        search_query = "token seguridad cajero generacion claves activacion"
        print(f"üîç Pregunta sobre token detectada, buscando: {search_query}")
    # Si es pregunta vaga Y hay contexto hist√≥rico, usar el contexto hist√≥rico para mejorar la b√∫squeda
    elif es_pregunta_vaga and context_historico:
        # Extraer t√©rminos clave del contexto hist√≥rico para mejorar la b√∫squeda
        # Buscar palabras clave relacionadas con el tema anterior
        if "token" in context_historico.lower() or "trabo" in context_historico.lower():
            search_query = "token seguridad cajero generacion claves activacion"
        elif "tasa" in context_historico.lower() or "interes" in context_historico.lower():
            search_query = "tasa interes deposito plazo fijo"
        else:
            # Buscar con t√©rminos del contexto hist√≥rico
            search_query = question + " " + " ".join(context_historico.lower().split()[:10])
        print(f"üîç Pregunta vaga detectada con contexto hist√≥rico, buscando: {search_query[:100]}")
    elif es_pregunta_vaga:
        # Si es pregunta vaga sin contexto, buscar con t√©rminos generales de token
        search_query = "token seguridad cajero generacion claves activacion"
        print(f"üîç Pregunta vaga detectada sin contexto, buscando: {search_query}")
    else:
        search_query = question
    
    # Buscar documentos relevantes - aumentar k para tener m√°s opciones
    # Para preguntas sobre cajero, buscar m√°s documentos porque el proceso completo puede estar en documentos m√°s abajo
    k_docs = 20 if pregunta_sobre_cajero else 8
    retriever = qdrant_docs.as_retriever(search_kwargs={"k": k_docs})
    relevant_docs = retriever.invoke(search_query)
    
    print(f"üîç Buscando documentos para: {question}")
    print(f"‚úÖ Encontr√© {len(relevant_docs)} documentos relevantes")
    
    # Mostrar preview de los primeros documentos para debugging
    if relevant_docs:
        for i, doc in enumerate(relevant_docs[:3]):
            preview = doc.page_content[:150] if hasattr(doc, 'page_content') else str(doc)[:150]
            print(f"   üìÑ Doc {i+1}: {preview}...")

    if relevant_docs and len(relevant_docs) > 0:
        # Filtrar documentos irrelevantes - solo usar documentos que mencionen palabras clave relacionadas
        palabras_clave_token = ["token", "cajero", "generaci√≥n", "generacion", "clave", "activaci√≥n", "activacion", "seguridad", "app macro", "banco macro"]
        palabras_clave_pregunta = question_lower.split()
        
        # Solo aplicar filtros estrictos para preguntas sobre ACCIONES espec√≠ficas (generar, activar, crear, nuevo token)
        # NO aplicar filtros estrictos para preguntas generales (qu√© es, c√≥mo funciona, para qu√© sirve)
        es_pregunta_accion_token = any(palabra in question_lower for palabra in ["generar", "nuevo token", "crear token", "activar token", "vencio", "venci√≥", "como genero", "como activo", "como creo"])
        es_pregunta_general = any(palabra in question_lower for palabra in ["qu√© es", "que es", "como funciona", "c√≥mo funciona", "para que", "para qu√©", "que es", "definicion", "definici√≥n"])
        
        context_parts = []
        for doc in relevant_docs:
            # Intentar obtener el texto del documento
            text = doc.page_content if hasattr(doc, 'page_content') and doc.page_content else None
            
            # Si no est√° en page_content, buscar en metadata
            if not text and hasattr(doc, 'metadata'):
                text = doc.metadata.get('text') or doc.metadata.get('payload', {}).get('text')
            
            # Si a√∫n no hay texto, usar el string del documento
            if not text:
                text = str(doc)
            
            if text and len(text.strip()) > 0:
                text_lower = text.lower()
                
                # Filtrar documentos irrelevantes SOLO cuando es necesario
                # Para preguntas generales (qu√© es, c√≥mo funciona), NO filtrar - usar todos los documentos relevantes
                if pregunta_sobre_cajero:
                    # Para preguntas sobre cajero autom√°tico, solo usar documentos que mencionen cajero autom√°tico
                    # Buscar documentos que tengan "Dirigite" o "cajero" + "banelco" o "cajero" + "tarjeta" + "claves"
                    tiene_dirigite_cajero = "dirigite" in text_lower and "cajero" in text_lower
                    tiene_cajero_banelco = "cajero" in text_lower and "banelco" in text_lower
                    tiene_cajero_tarjeta_claves = "cajero" in text_lower and ("tarjeta" in text_lower or "d√©bito" in text_lower or "debito" in text_lower) and ("claves" in text_lower or "generaci√≥n" in text_lower or "generacion" in text_lower)
                    
                    if not (tiene_dirigite_cajero or tiene_cajero_banelco or tiene_cajero_tarjeta_claves):
                        continue  # Saltar este documento si no menciona cajero autom√°tico con el proceso completo
                elif es_pregunta_accion_token and not es_pregunta_general:
                    # Solo para preguntas sobre ACCIONES espec√≠ficas (generar, activar, crear), filtrar documentos que no mencionen token
                    # Para preguntas generales, NO filtrar - dejar que el modelo use todos los documentos relevantes
                    if not any(palabra in text_lower for palabra in palabras_clave_token):
                        continue  # Saltar este documento si no menciona palabras clave de token
                # Para preguntas generales, NO filtrar - usar todos los documentos recuperados
                
                # Aumentar tama√±o permitido por documento para capturar m√°s contexto
                # Pero limitar para evitar contextos demasiado largos que confundan al modelo peque√±o
                context_parts.append(text.strip()[:1500])
        
        if context_parts:
            context = "\n\n---\n\n".join(context_parts)
            print(f"üìÑ Contexto construido con {len(context_parts)} documentos")
            print(f"üìè Longitud del contexto: {len(context)} caracteres")
            
            # Incluir contexto hist√≥rico en el prompt si existe
            contexto_completo = context
            if context_historico:
                contexto_completo = f"""CONTEXTO DE LA CONVERSACI√ìN ANTERIOR:
{context_historico}

---

CONTEXTO DE DOCUMENTOS DISPONIBLES:
{context}"""
            
            # Construir prompt espec√≠fico seg√∫n el tipo de pregunta
            if pregunta_sobre_cajero:
                # Prompt ultra-espec√≠fico para preguntas sobre cajero autom√°tico
                prompt = f"""Eres un asistente virtual del Banco Macro. El usuario pregunta espec√≠ficamente sobre ir al CAJERO AUTOM√ÅTICO.

{contexto_completo}

PREGUNTA ACTUAL DEL CLIENTE: {question}

REGLAS ULTRA-ESTRICTAS PARA PREGUNTAS SOBRE CAJERO AUTOM√ÅTICO:
1. Busca en el contexto la secci√≥n que dice "Dirigite a un Cajero Autom√°tico de la red Banelco" o "2) Dirigite"
2. Si encuentras esa secci√≥n, extrae SOLO los pasos que aparezcan en esa secci√≥n espec√≠fica sobre el cajero autom√°tico
3. Los pasos que DEBES mencionar (solo si est√°n en el contexto):
   - Dirigite a un Cajero Autom√°tico de la red Banelco
   - Ingres√° con tu tarjeta de d√©bito
   - Presion√° las siguientes opciones: Claves > Generaci√≥n de Claves > Token de Seguridad
   - Gener√° la Clave de Token (un c√≥digo de 6 d√≠gitos que deber√°s recordar)
   - El cajero emitir√° un comprobante con un C√≥digo de Activaci√≥n de 8 d√≠gitos (guarda este ticket)
4. PROHIBIDO ABSOLUTO mencionar:
   - Tel√©fono, llamar, 0810-555-2355
   - Sucursal, hablar con cajero, ir a sucursal
   - App, descargar, instalar, Google Play Store, App Store
   - Activar en el celular (paso 3)
   - Cualquier informaci√≥n sobre pagos, tasas, plazos fijos, u otros temas
5. Si el contexto menciona "1) Instal√° la App Macro" o "3) Activ√° el Token en tu celular", IGNORA completamente esas secciones
6. Si el contexto menciona m√∫ltiples pasos numerados (1), 2), 3)), SOLO usa el paso 2) sobre el cajero autom√°tico
7. Si NO encuentras informaci√≥n sobre el cajero autom√°tico en el contexto, di: "No tengo informaci√≥n espec√≠fica sobre el proceso en el cajero autom√°tico en los documentos disponibles."
8. Responde SIEMPRE en espa√±ol
9. S√© claro, conciso y numera los pasos (1), 2), 3), etc.) bas√°ndote SOLO en lo que aparece en el paso del cajero autom√°tico

RESPUESTA (SOLO pasos del cajero autom√°tico extra√≠dos del paso 2), NADA m√°s):"""
            else:
                # Prompt general para otras preguntas - m√°s flexible para preguntas generales
                if es_pregunta_general:
                    # Prompt m√°s flexible para preguntas generales (qu√© es, c√≥mo funciona, etc.)
                    prompt = f"""Eres un asistente virtual del Banco Macro. Responde la pregunta del cliente usando la informaci√≥n del contexto proporcionado.

{contexto_completo}

PREGUNTA ACTUAL DEL CLIENTE: {question}

INSTRUCCIONES:
1. Usa la informaci√≥n del contexto para responder la pregunta
2. Si la pregunta es sobre qu√© es algo o c√≥mo funciona, puedes inferir informaci√≥n b√°sica del contexto
3. Si mencionas pasos o procesos espec√≠ficos, aseg√∫rate de que est√©n en el contexto
4. NO inventes informaci√≥n sobre pasos espec√≠ficos, n√∫meros de tel√©fono, o procesos que no est√©n en el contexto
5. Responde SIEMPRE en espa√±ol
6. S√© claro y conciso

RESPUESTA:"""
                else:
                    # Prompt m√°s estricto para preguntas sobre acciones espec√≠ficas
                    prompt = f"""Eres un asistente virtual del Banco Macro. DEBES responder SOLO usando la informaci√≥n que est√° en el contexto proporcionado. NO inventes NADA.

{contexto_completo}

PREGUNTA ACTUAL DEL CLIENTE: {question}

REGLAS ESTRICTAS (LEE CUIDADOSAMENTE):
1. ANTES de responder, verifica que CADA PASO que menciones est√© EXPL√çCITAMENTE en el contexto proporcionado
2. Si la pregunta menciona "generar token", "nuevo token", "crear token", "activar token", "token vencido", "cambiar celular", "cambio de dispositivo":
   - Si el usuario YA TIENE la app instalada (dice "no me deja entrar", "ya tengo la app", "cambie de celular", "venci√≥ el token", etc.), NO menciones c√≥mo descargar la app
   - Enf√≥cate SOLO en los pasos de GENERACI√ìN/ACTIVACI√ìN del token que aparezcan en el contexto
   - Solo menciona pasos que veas en el contexto sobre: ir al CAJERO AUTOM√ÅTICO (no a la sucursal), generar claves, activar en el celular
   - NO inventes informaci√≥n sobre llamar por tel√©fono, ir a sucursal, o hablar con cajero a menos que est√© EXPL√çCITAMENTE en el contexto
   - Si el contexto habla de "instalar app" Y "generar token", SEPARA claramente: solo menciona la parte relevante seg√∫n la pregunta
3. Si la pregunta menciona "token trabo", "token bloqueado", "token no funciona":
   - Busca en el contexto informaci√≥n sobre "Token de Seguridad" y c√≥mo resolver problemas
   - Si encuentras informaci√≥n sobre desvincular, reinstalar o reactivar el token, √∫sala
   - Si no encuentras informaci√≥n espec√≠fica sobre ese problema, di qu√© informaci√≥n s√≠ tienes disponible
4. CR√çTICO: Si un documento en el contexto habla de OTROS temas (pagos, tasas, plazos fijos, etc.) y NO menciona "token", "cajero", "generaci√≥n de claves", o "activaci√≥n", NO uses esa informaci√≥n para responder
5. Si hay contexto hist√≥rico sobre "token", √∫salo para entender que el cliente ya estaba hablando del Token de Seguridad
6. Responde DIRECTAMENTE la pregunta usando SOLO la informaci√≥n del contexto que sea RELEVANTE
7. NO inventes pasos, procesos o informaci√≥n que no est√© en el contexto
8. NO mezcles informaci√≥n de diferentes temas (pagos, tasas, etc.) con informaci√≥n sobre token
9. NO agregues frases como "No tengo m√°s informaci√≥n" o "No s√©" al final si ya diste una respuesta √∫til con los pasos disponibles
10. Si NO encuentras informaci√≥n relevante en absoluto, di: "No tengo esa informaci√≥n espec√≠fica en los documentos disponibles."
11. Responde SIEMPRE en espa√±ol
12. S√© claro, conciso y estructurado. Si hay pasos, num√©ralos claramente (1), 2), 3), etc.)

RESPUESTA (usa SOLO informaci√≥n del contexto que sea relevante a la pregunta):"""
        else:
            print("‚ö†Ô∏è Los documentos encontrados no tienen contenido v√°lido")
            prompt = f"""Eres un asistente virtual del Banco Macro. 

PREGUNTA DEL CLIENTE: {question}

Si no tienes informaci√≥n sobre esta pregunta, responde EXACTAMENTE: "No tengo esa informaci√≥n espec√≠fica en los documentos disponibles."

RESPUESTA:"""
    else:
        print("‚ö†Ô∏è No se encontraron documentos relevantes")
        prompt = f"""Eres un asistente virtual del Banco Macro. 

PREGUNTA DEL CLIENTE: {question}

No tengo informaci√≥n espec√≠fica en los documentos disponibles para responder esta pregunta. Responde de forma breve y profesional indicando esto.

RESPUESTA:"""
    
    response = llm.invoke(prompt)
    answer = response 

    redis_cache.cache_answer(question, answer)
    
    redis_cache.save_to_conversation(thread_id, question, answer)

    try:
        if qdrant_conversations is not None and bool(qdrant_conversations):
            qdrant_conversations.add_texts([question, answer])
    except Exception as e:
        print(f"Advertencia: No se pudo guardar en Qdrant: {e}")

    return answer

